import io
import os
import shutil
from datetime import date, timedelta
from pathlib import Path
from typing import Optional

import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

from model_utils import (
    list_models,
    load_model,
    predict_price,
    clean_marginal_data,
    train_model_ex,
    load_model_meta,
    delete_models,
    get_dataset_status,
    preprocess_dataset_and_sync,
    fetch_dataset,
    format_time_shanghai,
)

from scripts.pre_process import preprocess_data, preprocess_template_file


def _parse_date(value: Optional[str]) -> Optional[date]:
    if not value:
        return None
    try:
        return date.fromisoformat(value)
    except ValueError:
        return None


def _altair_chart(chart, *, width: str = "stretch") -> None:
    try:
        st.altair_chart(chart, width=width)
    except TypeError:
        # Backward-compatible fallback for older Streamlit versions.
        st.altair_chart(chart, use_container_width=(width == "stretch"))


REQUIRED_BOUNDARY_FILES = [
    "æ—¥å‰ç»Ÿè°ƒç³»ç»Ÿè´Ÿè·é¢„æµ‹_REPORT0.xlsx",
    "æ—¥å‰æ–°èƒ½æºè´Ÿè·é¢„æµ‹_REPORT0.xlsx",
    "æŠ«éœ²ä¿¡æ¯96ç‚¹æ•°æ®_REPORT0.xlsx",
    "æ—¥å‰è”ç»œçº¿è®¡åˆ’_REPORT0.xlsx",
    "æ—¥å‰å¸‚åœºå‡ºæ¸…æƒ…å†µ_TABLE.xlsx",
    "æ—¥å‰æ°´ç”µè®¡åˆ’å‘ç”µæ€»å‡ºåŠ›é¢„æµ‹_REPORT0.xlsx",
    "96ç‚¹ç”µç½‘è¿è¡Œå®é™…å€¼_REPORT0.xlsx",
    "å®æ—¶è”ç»œçº¿è®¡åˆ’_REPORT0.xlsx",
    "ç°è´§å‡ºæ¸…ç”µä»·_REPORT0.xlsx",
]

PUBLIC_REALTIME_CAPACITY_LABEL = "å…¬æœ‰æ•°æ®çœ‹æ¿-å®æ—¶(å¤©é™…äº‘).xlsx"


def _df_to_excel_bytes(df: pd.DataFrame, *, sheet_name: str = "åˆå¹¶æ•°æ®") -> bytes:
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet_name)
    buffer.seek(0)
    return buffer.getvalue()


def _clear_directory(path: Path) -> None:
    if not path.exists():
        return
    for child in path.iterdir():
        if child.is_dir():
            shutil.rmtree(child)
        else:
            child.unlink()


def _persist_uploaded_files(files, dest_dir: Path) -> None:
    stored_files = []
    for file in files:
        file.seek(0)
        stored_files.append((file.name, file.getvalue()))
    _clear_directory(dest_dir)
    for name, content in stored_files:
        with open(dest_dir / name, "wb") as f:
            f.write(content)


st.set_page_config(page_title="CoPiLinear", layout="wide")
st.title("CoPiLinear ä»·æ ¼é¢„æµ‹å·¥å…·")

MENU = st.sidebar.radio("å¯¼èˆª", ["æ•°æ®é¢„å¤„ç†", "æ¨¡å‹ç®¡ç†", "æ¨¡å‹è®­ç»ƒ", "æ—¥å‰ä»·æ ¼é¢„æµ‹"])


# ----------------------------------------
# 0) æ•°æ®é¢„å¤„ç†
# ----------------------------------------
if MENU == "æ•°æ®é¢„å¤„ç†":
    st.subheader("é¢„å¤„ç†è¾¹ç•Œæ•°æ®")
    st.caption("ä¸Šä¼  10 ä¸ªè¾¹ç•Œ Excel æ–‡ä»¶æˆ– 0å¸‚åœºè¾¹é™…æ•°æ®åº“.xlsx æ¨¡æ¿ï¼Œå®Œæˆæ•°æ®æ¸…æ´—å¹¶å¯¼å…¥æ•°æ®åº“ã€‚")

    margin_dir = Path("margin_data")
    margin_dir.mkdir(parents=True, exist_ok=True)

    import_mode = st.radio(
        "å¯¼å…¥æ–¹å¼",
        ["é€æ–‡ä»¶åˆå¹¶ï¼ˆ10 ä¸ªæ–‡ä»¶ï¼‰", "æ¨¡æ¿æ‰¹é‡å¯¼å…¥ (0å¸‚åœºè¾¹é™…æ•°æ®åº“.xlsx)"],
        horizontal=True,
        key="preprocess_import_mode",
    )

    boundary_success: Optional[str] = None
    boundary_error: Optional[str] = None

    if import_mode == "é€æ–‡ä»¶åˆå¹¶ï¼ˆ10 ä¸ªæ–‡ä»¶ï¼‰":
        st.markdown("### ğŸ“¤ ä¸Šä¼ è¾¹ç•Œæ•°æ®æ–‡ä»¶")
        st.warning("âš ï¸ è¯·ä¸Šä¼ ä»¥ä¸‹ 10 ä¸ªå¿…éœ€çš„ Excel æ–‡ä»¶ï¼š")
        st.markdown(
            """
1. æ—¥å‰ç»Ÿè°ƒç³»ç»Ÿè´Ÿè·é¢„æµ‹_REPORT0.xlsx  
2. æ—¥å‰æ–°èƒ½æºè´Ÿè·é¢„æµ‹_REPORT0.xlsx  
3. æŠ«éœ²ä¿¡æ¯96ç‚¹æ•°æ®_REPORT0.xlsx  
4. æ—¥å‰è”ç»œçº¿è®¡åˆ’_REPORT0.xlsx  
5. æ—¥å‰å¸‚åœºå‡ºæ¸…æƒ…å†µ_TABLE.xlsx  
6. æ—¥å‰æ°´ç”µè®¡åˆ’å‘ç”µæ€»å‡ºåŠ›é¢„æµ‹_REPORT0.xlsx  
7. 96ç‚¹ç”µç½‘è¿è¡Œå®é™…å€¼_REPORT0.xlsx  
8. å®æ—¶è”ç»œçº¿è®¡åˆ’_REPORT0.xlsx  
9. ç°è´§å‡ºæ¸…ç”µä»·_REPORT0.xlsx  
10. å…¬æœ‰æ•°æ®çœ‹æ¿-å®æ—¶ï¼ˆæ–‡ä»¶ååŒ…å«æ—¥æœŸèŒƒå›´ï¼‰
            """
        )

        boundary_files = st.file_uploader(
            "é€‰æ‹©Excelæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
            type=["xlsx"],
            accept_multiple_files=True,
            help="è¯·ä¸€æ¬¡æ€§ä¸Šä¼ å…¨éƒ¨ 10 ä¸ªæ–‡ä»¶ï¼ˆå«å…¬æœ‰æ•°æ®çœ‹æ¿-å®æ—¶ï¼‰",
            key="boundary_file_uploader",
        )

        required_status = {name: False for name in REQUIRED_BOUNDARY_FILES}
        capacity_file_found = False
        if boundary_files:
            st.markdown(f"âœ… å·²é€‰æ‹© {len(boundary_files)} ä¸ªæ–‡ä»¶ï¼š")
            for file in boundary_files:
                st.write(f"  - {file.name}")
                if file.name in required_status:
                    required_status[file.name] = True
                if file.name.startswith("å…¬æœ‰æ•°æ®çœ‹æ¿-å®æ—¶"):
                    capacity_file_found = True

        missing_files = [name for name, found in required_status.items() if not found]
        if not capacity_file_found:
            missing_files.append(PUBLIC_REALTIME_CAPACITY_LABEL)

        # æ˜¾ç¤ºæ–‡ä»¶çŠ¶æ€ä¿¡æ¯
        if boundary_files:
            found_files = [name for name, found in required_status.items() if found]
            if capacity_file_found:
                found_files.append("å…¬æœ‰æ•°æ®çœ‹æ¿-å®æ—¶æ–‡ä»¶")

            if missing_files:
                st.info(f"ğŸ“Š å·²ä¸Šä¼  {len(found_files)} ä¸ªæ–‡ä»¶ï¼Œè¿˜å¯é€‰æ‹©ä¸Šä¼  {len(missing_files)} ä¸ªæ–‡ä»¶ï¼š")
                for name in missing_files:
                    st.write(f"  - {name}")
                st.caption("ğŸ’¡ æç¤ºï¼šå¯ä»¥ä½¿ç”¨éƒ¨åˆ†æ–‡ä»¶è¿›è¡Œåˆå¹¶ï¼Œç³»ç»Ÿä¼šæ ¹æ®å¯ç”¨æ–‡ä»¶ç”Ÿæˆç›¸åº”çš„æ•°æ®ã€‚")
            else:
                st.success("âœ… æ‰€æœ‰æ¨èæ–‡ä»¶å·²ä¸Šä¼ ï¼")

        force_sync = st.checkbox("å¿½ç•¥ç¼“å­˜å¼ºåˆ¶å¯¼å…¥", value=False, key="boundary_force_sync")

        run_clicked = st.button(
            "ğŸ”„ ä¿å­˜æ–‡ä»¶å¹¶å¯¼å…¥æ•°æ®åº“",
            type="primary",
            key="boundary_process",
            disabled=(not boundary_files),  # åªè¦æœ‰æ–‡ä»¶å°±å¯ä»¥å¤„ç†
        )
        merge_only_clicked = st.button(
            "âš™ï¸ ä»…åˆå¹¶ç”Ÿæˆé¢„å¤„ç†ç»“æœ (ä¸å¯¼å…¥æ•°æ®åº“)",
            key="boundary_merge_only",
            disabled=(not boundary_files),  # åªè¦æœ‰æ–‡ä»¶å°±å¯ä»¥å¤„ç†
        )

        if run_clicked or merge_only_clicked:
            if not boundary_files:
                boundary_error = "è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶åå†å¤„ç†"
            else:
                try:
                    _persist_uploaded_files(boundary_files, margin_dir)
                    spinner_msg = (
                        "æ­£åœ¨è¿è¡Œé¢„å¤„ç†è„šæœ¬å¹¶å†™å…¥æ•°æ®åº“..."
                        if run_clicked
                        else "æ­£åœ¨è¿è¡Œé¢„å¤„ç†è„šæœ¬..."
                    )
                    with st.spinner(spinner_msg):
                        preview_df = preprocess_data(data_dir=margin_dir, verbose=False)
                        sync_result = None
                        if run_clicked:
                            sync_result = preprocess_dataset_and_sync(
                                force=force_sync,
                                data_dir=margin_dir,
                                source_label="multi_file_bundle",
                            )

                    st.session_state["boundary_result"] = preview_df
                    st.session_state["boundary_filename"] = "é¢„å¤„ç†ç»“æœ_æ–°ç‰ˆ.xlsx"
                    if run_clicked and sync_result is not None:
                        boundary_success = f"{sync_result['status']}ï¼Œå¯¼å…¥ {sync_result['rows']:,} è¡Œæ•°æ®"
                    else:
                        boundary_success = f"åˆå¹¶å®Œæˆï¼Œå…± {len(preview_df):,} è¡Œæ•°æ®ï¼Œå¯ç›´æ¥ä¸‹è½½"
                except Exception as exc:
                    boundary_error = str(exc)

    else:
        st.markdown("### ğŸ“¥ ä¸Šä¼ æ¨¡æ¿ (0å¸‚åœºè¾¹é™…æ•°æ®åº“.xlsx)")
        template_file = st.file_uploader(
            "é€‰æ‹©æ¨¡æ¿ Excel",
            type=["xlsx"],
            accept_multiple_files=False,
            key="template_file_uploader",
        )
        force_sync_template = st.checkbox("å¿½ç•¥ç¼“å­˜å¼ºåˆ¶å¯¼å…¥", value=False, key="template_force_sync")
        template_import = st.button(
            "ğŸ”„ å¯¼å…¥æ¨¡æ¿åˆ°æ•°æ®åº“",
            type="primary",
            key="template_import",
            disabled=template_file is None,
        )
        template_merge = st.button(
            "âš™ï¸ ä»…åˆå¹¶æ¨¡æ¿ (ä¸å¯¼å…¥æ•°æ®åº“)",
            key="template_merge",
            disabled=template_file is None,
        )

        if template_import or template_merge:
            if not template_file:
                boundary_error = "è¯·å…ˆä¸Šä¼ æ¨¡æ¿æ–‡ä»¶"
            else:
                try:
                    _persist_uploaded_files([template_file], margin_dir)
                    template_path = margin_dir / template_file.name
                    spinner_msg = (
                        "æ­£åœ¨å¤„ç†æ¨¡æ¿å¹¶å†™å…¥æ•°æ®åº“..."
                        if template_import
                        else "æ­£åœ¨å¤„ç†æ¨¡æ¿..."
                    )
                    with st.spinner(spinner_msg):
                        template_df = preprocess_template_file(template_path, verbose=False)
                        sync_result = None
                        if template_import:
                            sync_result = preprocess_dataset_and_sync(
                                force=force_sync_template,
                                preprocessed_df=template_df,
                                source_label="template_file",
                            )

                    st.session_state["boundary_result"] = template_df
                    st.session_state["boundary_filename"] = template_file.name or "é¢„å¤„ç†ç»“æœ_æ–°ç‰ˆ.xlsx"
                    if template_import and sync_result is not None:
                        boundary_success = f"{sync_result['status']}ï¼Œå¯¼å…¥ {sync_result['rows']:,} è¡Œæ•°æ®"
                    else:
                        boundary_success = f"æ¨¡æ¿åˆå¹¶å®Œæˆï¼Œå…± {len(template_df):,} è¡Œï¼Œå¯ç›´æ¥ä¸‹è½½"
                except Exception as exc:
                    boundary_error = str(exc)

    if boundary_success:
        st.success(boundary_success)
    if boundary_error:
        st.error(boundary_error)

    if "boundary_result" in st.session_state:
        result_df = st.session_state["boundary_result"]
        st.markdown("### ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
        col1, col2, col3 = st.columns(3)
        col1.metric("æ€»è¡Œæ•°", len(result_df))
        col2.metric(
            "æ—¥å‰æ•°æ®è¡Œæ•°",
            len(result_df[result_df["è¾¹ç•Œæ•°æ®ç±»å‹"] == "æ—¥å‰"]),
        )
        col3.metric(
            "å®æ—¶æ•°æ®è¡Œæ•°",
            len(result_df[result_df["è¾¹ç•Œæ•°æ®ç±»å‹"] == "å®æ—¶"]),
        )

        if "åœ¨çº¿æœºç»„å®¹é‡(MW)" in result_df.columns:
            online_capacity = result_df["åœ¨çº¿æœºç»„å®¹é‡(MW)"].dropna()
            cap_val = (
                online_capacity.iloc[0]
                if not online_capacity.empty
                else "æœªæ‰¾åˆ°"
            )
            st.info(f"ğŸ’¡ æå–åˆ°åœ¨çº¿æœºç»„å®¹é‡: {cap_val}")

        st.markdown("### ğŸ‘€ æ•°æ®é¢„è§ˆ")
        st.dataframe(result_df.head(30), use_container_width=True)

        st.markdown("### ğŸ“¥ ä¸‹è½½é¢„å¤„ç†ç»“æœ")
        excel_data = _df_to_excel_bytes(result_df, sheet_name="é¢„å¤„ç†æ•°æ®")
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½é¢„å¤„ç†åçš„Excelæ–‡ä»¶",
            data=excel_data,
            file_name=st.session_state.get("boundary_filename", "é¢„å¤„ç†ç»“æœ_æ–°ç‰ˆ.xlsx"),
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    st.markdown("---")
    st.markdown("### æ•°æ®åº“çŠ¶æ€")
    dataset_status = get_dataset_status()
    c1, c2, c3 = st.columns(3)
    c1.metric("çŠ¶æ€", dataset_status["state_label"], dataset_status.get("status_reason") or "")
    c2.metric("æ•°æ®åº“è¡Œæ•°", f"{dataset_status['row_count']:,}")
    c3.metric("ä¸Šæ¬¡åŒæ­¥", dataset_status.get("last_sync_time") or "â€”")
    st.caption(
        f"å¯ç”¨æ•°æ®åŒºé—´ï¼š{dataset_status.get('date_min') or 'â€”'} è‡³"
        f" {dataset_status.get('date_max') or 'â€”'}"
    )
    if dataset_status.get("data_dir"):
        st.caption(f"æœ€è¿‘å¯¼å…¥ç›®å½•ï¼š{dataset_status['data_dir']}")
    if dataset_status.get("raw_row_count") is not None:
        st.caption(f"åŸå§‹è¡¨æ¡ç›®ï¼š{dataset_status['raw_row_count']:,}")

    st.caption("ğŸ’¡ æç¤ºï¼šä¸Šä¼ çš„æ–‡ä»¶ä»…ç”¨äºå½“å‰ä¼šè¯ï¼Œä¸ä¼šè¢«æ°¸ä¹…ä¿å­˜ã€‚")


# ----------------------------------------
# 1) æ¨¡å‹ç®¡ç†
# ----------------------------------------
elif MENU == "æ¨¡å‹ç®¡ç†":
    st.subheader("æ¨¡å‹æ•°æ®åº“")
    all_models = list_models()
    if not all_models:
        st.info("æš‚æ— æ¨¡å‹ï¼Œè¯·å…ˆåˆ°â€œæ¨¡å‹è®­ç»ƒâ€é¡µåˆ›å»ºæ¨¡å‹ã€‚")
    else:
        # æ„å»ºæ±‡æ€»è¡¨
        records = []
        for name in all_models:
            meta = load_model_meta(name) or {}
            pkl_path = os.path.join("models", f"{name}.pkl")
            meta_path = os.path.join("models", f"{name}.meta.json")
            train_path = os.path.join("models", f"{name}.train.csv")
            rec = {
                "é€‰ä¸­": False,
                "åç§°": name,
                "åˆ›å»ºæ—¶é—´": format_time_shanghai(meta.get("created_at")),
                "åˆ†æ®µæ•°": meta.get("n_segments"),
                "æ ·æœ¬è¡Œæ•°": meta.get("rows"),
                "æ•°æ®æ¥æº": meta.get("source"),
                "æ–­ç‚¹": ", ".join([f"{v:.4f}" for v in meta.get("breakpoints", [])]) if meta.get("breakpoints") else None,
            }
            records.append(rec)
        df_models = pd.DataFrame.from_records(records)

        # é¡¶éƒ¨å·¥å…·æ¡
        c1, c2, c3, c4 = st.columns([2, 2, 2, 3])
        with c1:
            q = st.text_input("æœç´¢åç§°", value="").strip()
        with c2:
            seg_options = sorted([v for v in df_models["åˆ†æ®µæ•°"].dropna().unique().tolist()])
            seg_filter = st.multiselect("åˆ†æ®µæ•°ç­›é€‰", seg_options, default=[])
        # è¿‡æ»¤
        view_df = df_models.copy()
        if q:
            view_df = view_df[view_df["åç§°"].str.contains(q, case=False, na=False)]
        if seg_filter:
            view_df = view_df[view_df["åˆ†æ®µæ•°"].isin(seg_filter)]
            
        # åˆ—é…ç½®ï¼šä»…â€œé€‰ä¸­â€å¯ç¼–è¾‘
        col_cfg = {
            "é€‰ä¸­": st.column_config.CheckboxColumn("é€‰ä¸­", help="å‹¾é€‰ä»¥åŠ å…¥æ‰¹é‡æ“ä½œ"),
            "åç§°": st.column_config.TextColumn("åç§°", disabled=True),
            "åˆ›å»ºæ—¶é—´": st.column_config.TextColumn("åˆ›å»ºæ—¶é—´", disabled=True),
            "åˆ†æ®µæ•°": st.column_config.NumberColumn("åˆ†æ®µæ•°", disabled=True),
            "æ ·æœ¬è¡Œæ•°": st.column_config.NumberColumn("æ ·æœ¬è¡Œæ•°", disabled=True),
            "æ•°æ®æ¥æº": st.column_config.TextColumn("æ•°æ®æ¥æº", disabled=True),
            "æ–­ç‚¹": st.column_config.TextColumn("æ–­ç‚¹", disabled=True),
        }

        edited = st.data_editor(
            view_df,
            hide_index=True,
            width="stretch",
            column_config=col_cfg,
        )

        # å½“å‰é€‰æ‹©
        chosen_names = []
        if "é€‰ä¸­" in edited.columns:
            chosen_names = edited.loc[edited["é€‰ä¸­"] == True, :]["åç§°"].tolist()

        st.markdown("---")
        # è¯¦æƒ…é¢„è§ˆï¼ˆå•é€‰æ—¶å±•ç¤ºï¼‰
        if len(chosen_names) == 1:
            name = chosen_names[0]
            st.subheader(f"è¯¦æƒ…ï¼š{name}")
            meta = load_model_meta(name)
            if meta:
                st.write(f"åˆ›å»ºæ—¶é—´: {format_time_shanghai(meta.get('created_at'))}")
                bps = meta.get('breakpoints')
                if bps is not None:
                    st.write(f"æ–­ç‚¹: {np.round(np.array(bps), 4)}")
                st.write(f"è®­ç»ƒæ•°æ®æ¥æº: {meta.get('source')}")
                st.write(f"æ ·æœ¬è¡Œæ•°: {meta.get('rows')}")
            else:
                st.info("ç¼ºå°‘å…ƒæ•°æ®ï¼ˆmetaï¼‰ï¼Œä»…æä¾›æ¨¡å‹æ–‡ä»¶æ“ä½œã€‚")

            c1, c2, c3 = st.columns(3)
            with c1:
                try:
                    with open(f"models/{name}.pkl", "rb") as f:
                        st.download_button("ä¸‹è½½æ¨¡å‹(.pkl)", f, file_name=f"{name}.pkl")
                except Exception:
                    st.warning("æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
            with c2:
                try:
                    with open(f"models/{name}.train.csv", "rb") as f:
                        st.download_button("ä¸‹è½½è®­ç»ƒæ•°æ®", f, file_name=f"{name}.train.csv")
                except Exception:
                    st.info("æ— è®­ç»ƒæ•°æ®æ–‡ä»¶")
            with c3:
                if st.button(f"åˆ é™¤ {name}"):
                    delete_models([name])
                    st.success(f"å·²åˆ é™¤ {name}")

            try:
                train_df = pd.read_csv(f"models/{name}.train.csv")
                st.write(f"è®­ç»ƒæ•°æ®è¡Œæ•°: {len(train_df):,}")
                st.dataframe(train_df.head(10))
            except Exception:
                pass


# ----------------------------------------
# 2) æ¨¡å‹è®­ç»ƒ
# ----------------------------------------
elif MENU == "æ¨¡å‹è®­ç»ƒ":
    st.subheader("æ•°æ®æºæ¦‚è§ˆ")
    dataset_status = get_dataset_status()

    c1, c2, c3 = st.columns(3)
    c1.metric("çŠ¶æ€", dataset_status["state_label"], dataset_status.get("status_reason") or "")
    c2.metric("æ•°æ®åº“è¡Œæ•°", f"{dataset_status['row_count']:,}")
    c3.metric("ä¸Šæ¬¡åŒæ­¥", dataset_status.get("last_sync_time") or "â€”")
    st.caption(
        f"æ•°æ®åŒºé—´ï¼š{dataset_status.get('date_min') or 'â€”'} è‡³"
        f" {dataset_status.get('date_max') or 'â€”'}"
    )
    if dataset_status.get("data_dir"):
        st.caption(f"æœ€è¿‘ä½¿ç”¨çš„æ•°æ®ç›®å½•ï¼š{dataset_status['data_dir']}")
    st.info("è‹¥éœ€åˆ·æ–°æ•°æ®åº“ï¼Œè¯·å‰å¾€â€œæ•°æ®é¢„å¤„ç†â€é¡µè¿è¡Œè„šæœ¬ã€‚")

    st.markdown("---")
    st.subheader("æ¨¡å‹è®­ç»ƒ")
    data_source = st.radio("è®­ç»ƒæ•°æ®æ¥æº", ["ä¸Šä¼ æ–‡ä»¶", "æ•°æ®åº“"], horizontal=True)

    upload_file = None
    db_start: Optional[date] = None
    db_end: Optional[date] = None

    if data_source == "ä¸Šä¼ æ–‡ä»¶":
        upload_file = st.file_uploader("ä¸Šä¼ è®­ç»ƒæ•°æ® Excel/CSV", type=["xlsx", "csv"])
    else:
        if dataset_status["row_count"] == 0:
            st.info("æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆåˆ°â€œæ•°æ®é¢„å¤„ç†â€é¡µå¯¼å…¥æ•°æ®ã€‚")
        min_date = _parse_date(dataset_status.get("date_min"))
        max_date = _parse_date(dataset_status.get("date_max"))
        today = date.today()
        default_end = max_date or today
        default_start = default_end - timedelta(days=30)
        if min_date:
            default_start = max(min_date, default_start)
        cols = st.columns(2)
        db_start = cols[0].date_input(
            "èµ·å§‹æ—¥æœŸ",
            value=default_start,
            min_value=min_date or default_start,
            max_value=default_end,
        )
        db_end = cols[1].date_input(
            "ç»“æŸæ—¥æœŸ",
            value=default_end,
            min_value=db_start,
            max_value=max_date or default_end,
        )
        st.caption(
            f"å¯ç”¨æ•°æ®åŒºé—´ï¼š{dataset_status.get('date_min') or 'æœªçŸ¥'} è‡³ {dataset_status.get('date_max') or 'æœªçŸ¥'}"
        )

    model_name = st.text_input("æ¨¡å‹åç§°", value="model_v1").strip()
    n_segments = st.slider("åˆ†æ®µæ•° (pwlf)", min_value=2, max_value=8, value=3)

    if st.button("è®­ç»ƒæ¨¡å‹"):
        df_source = None
        source_label = None

        if data_source == "ä¸Šä¼ æ–‡ä»¶":
            if not upload_file:
                st.warning("è¯·å…ˆä¸Šä¼ è®­ç»ƒæ•°æ®æ–‡ä»¶")
            else:
                suffix = upload_file.name.lower()
                if suffix.endswith(".xlsx") or suffix.endswith(".xls"):
                    df_source = pd.read_excel(upload_file, header=None)
                else:
                    df_source = pd.read_csv(upload_file, header=None)
                source_label = upload_file.name
        else:
            if dataset_status["row_count"] == 0:
                st.warning("æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®")
            elif not db_start or not db_end:
                st.warning("è¯·é€‰æ‹©å®Œæ•´çš„æ—¥æœŸèŒƒå›´")
            elif db_start > db_end:
                st.warning("èµ·å§‹æ—¥æœŸéœ€æ—©äºç»“æŸæ—¥æœŸ")
            else:
                df = fetch_dataset(db_start, db_end)
                if df.empty:
                    st.warning("æ‰€é€‰æ—¥æœŸèŒƒå›´æ²¡æœ‰æ•°æ®")
                else:
                    df_source = df
                    source_label = f"sqlite:{db_start}->{db_end}"

        if df_source is None:
            st.stop()
        if not model_name:
            st.warning("è¯·å¡«å†™æ¨¡å‹åç§°")
            st.stop()

        try:
            model, breakpoints = train_model_ex(
                df_source,
                model_name=model_name,
                n_segments=n_segments,
                source=source_label,
            )
            st.success(
                f"è®­ç»ƒå®Œæˆï¼š{model_name}ï¼Œæ–­ç‚¹ï¼š{np.round(breakpoints, 4)}ï¼Œæ¥æºï¼š{source_label or 'unknown'}"
            )

            cleaned = clean_marginal_data(df_source.copy())
            scatter_df = cleaned[["load_rate", "price"]].copy()
            x_line = np.linspace(scatter_df["load_rate"].min(), scatter_df["load_rate"].max(), 200)
            line_df = pd.DataFrame(
                {
                    "load_rate": x_line,
                    "price": predict_price(model, x_line),
                }
            )
            bp_df = pd.DataFrame({"load_rate": np.asarray(breakpoints)})

            points = (
                alt.Chart(scatter_df)
                .mark_point(opacity=0.4, size=35)
                .encode(
                    x=alt.X("load_rate:Q", title="è´Ÿè·ç‡(%)"),
                    y=alt.Y("price:Q", title="ä»·æ ¼"),
                )
            )
            line = (
                alt.Chart(line_df)
                .mark_line(color="#d62728", strokeWidth=2)
                .encode(x="load_rate:Q", y="price:Q")
            )
            rules = (
                alt.Chart(bp_df)
                .mark_rule(color="#7f7f7f", strokeDash=[4, 4])
                .encode(x="load_rate:Q")
            )

            _altair_chart((points + line + rules).properties(title="è´Ÿè·ç‡æ‹Ÿåˆå›¾"), width="stretch")
        except Exception as e:
            st.error(f"è®­ç»ƒå¤±è´¥ï¼š{e}")


# ----------------------------------------
# 3) æ—¥å‰ä»·æ ¼é¢„æµ‹
# ----------------------------------------
elif MENU == "æ—¥å‰ä»·æ ¼é¢„æµ‹":
    st.subheader("å¤šæ¨¡å‹æ—¥å‰ä»·æ ¼é¢„æµ‹")

    models = list_models()
    if not models:
        st.warning("æš‚æ— å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆåœ¨â€œæ¨¡å‹è®­ç»ƒâ€é¡µè®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ã€‚")
        st.stop()
    selected_models = st.multiselect("é€‰æ‹©æ¨¡å‹", models, default=models[:1])
    if not selected_models:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
        st.stop()

    cap_input = st.text_input("æ—¥å‰åœ¨çº¿æœºç»„å®¹é‡ (MWï¼Œå¤šå€¼ç”¨é€—å·/ç©ºæ ¼åˆ†éš”)", value="45000, 50000, 55000")
    capacities = []
    for part in cap_input.replace("ï¼Œ", ",").replace(" ", ",").split(","):
        p = part.strip()
        if not p:
            continue
        try:
            v = float(p)
        except ValueError:
            st.error(f"å®¹é‡å€¼æ— æ³•è§£æï¼š'{p}'")
            st.stop()
        if v <= 0:
            st.error("å®¹é‡å¿…é¡»å¤§äº 0 MW")
            st.stop()
        capacities.append(v)
    if not capacities:
        st.info("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæ­£çš„å®¹é‡å€¼")

    file = st.file_uploader("ä¸Šä¼ åŒ…å«â€œç«ä»·ç©ºé—´â€åˆ—çš„ CSV/XLSX", type=["csv", "xlsx"])
    if st.button("é¢„æµ‹") and file:
        df = pd.read_csv(file) if file.name.lower().endswith(".csv") else pd.read_excel(file)
        if not capacities:
            st.error("å®¹é‡ä¸èƒ½ä¸ºç©º")
            st.stop()
        # è§„èŒƒåŒ–åˆ—ååæŸ¥æ‰¾â€œç«ä»·ç©ºé—´â€
        def _normalize_col(col: str) -> str:
            s = str(col)
            for ch in [" ", "\u3000", "\t", "\n", "\r"]:
                s = s.replace(ch, "")
            return s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")

        norm_map = {_normalize_col(c): c for c in df.columns}
        space_col = None
        for cand in ["ç«ä»·ç©ºé—´(MW)", "ç«ä»·ç©ºé—´"]:
            norm_cand = _normalize_col(cand)
            if norm_cand in norm_map:
                space_col = norm_map[norm_cand]
                break
        if space_col is None:
            fuzzy = [orig for norm, orig in norm_map.items() if ("ç«ä»·" in norm) and ("ç©ºé—´" in norm)]
            if fuzzy:
                space_col = fuzzy[0]
        if space_col is None:
            st.error(f"æœªæ‰¾åˆ°åŒ…å«â€œç«ä»·ç©ºé—´â€çš„åˆ—ï¼Œå½“å‰åˆ—åï¼š{list(df.columns)}")
            st.stop()

        space_vals = pd.to_numeric(df[space_col], errors="coerce").values
        if np.isnan(space_vals).all():
            st.error("â€œç«ä»·ç©ºé—´â€åˆ—æ— æ³•è§£æä¸ºæ•°å€¼ï¼Œè¯·æ£€æŸ¥æ•°æ®")
            st.stop()

        # ç”Ÿæˆ 15 åˆ†é’Ÿç²’åº¦æ—¶é—´æ ‡ç­¾ï¼ˆè‹¥ä¸º 96 æ¡åˆ™æœ€åä¸º 24:00ï¼‰
        _mins = np.arange(1, len(space_vals) + 1) * 15
        _hrs = _mins // 60
        _mm = _mins % 60
        time_labels = [f"{int(h):02d}:{int(m):02d}" for h, m in zip(_hrs, _mm)]

        # å¤šå®¹é‡é¢„æµ‹ï¼šä¸ºæ¯ä¸ªå®¹é‡è®¡ç®—è´Ÿè·ç‡å’Œå„æ¨¡å‹ä»·æ ¼
        display_df = pd.DataFrame({"time_slot": time_labels})
        for cap in capacities:
            load_rate = (space_vals / cap) * 100.0
            display_df[f"è´Ÿè·ç‡(%)@{cap}MW"] = load_rate
            for name in selected_models:
                m = load_model(name)
                if m is None:
                    st.warning(f"æ¨¡å‹ '{name}' åŠ è½½å¤±è´¥ï¼Œå·²è·³è¿‡")
                    continue
                preds = predict_price(m, load_rate)
                display_df[f"{name}@{cap}MW"] = preds
        st.dataframe(display_df.head())

        # ä¸‹è½½ç»“æœï¼šåŒ…å« time_slotã€å„å®¹é‡å¯¹åº”è´Ÿè·ç‡ã€å„æ¨¡å‹é¢„æµ‹ä»·æ ¼
        download_df = display_df.copy()
        st.download_button(
            "ä¸‹è½½é¢„æµ‹ç»“æœ",
            download_df.to_csv(index=False).encode(),
            file_name="price_predictions.csv",
        )

        # å›¾è¡¨ï¼šä»…æ˜¾ç¤ºå„æ¨¡å‹é¢„æµ‹ï¼Œä¸æ˜¾ç¤ºå®¹é‡ä¸è´Ÿè·ç‡ï¼›Y è½´é”å®š 0-1500ï¼ŒX è½´åŒ…å« 24:00
        model_cols = [c for c in download_df.columns if c not in ["time_slot"] and not c.startswith("è´Ÿè·ç‡(%)@")]
        chart_df = download_df[["time_slot", *model_cols]].melt('time_slot', var_name='model', value_name='price')
        hour_ticks = [t for t in time_labels if t.endswith(':00')]
        if '24:00' in time_labels and '24:00' not in hour_ticks:
            hour_ticks.append('24:00')
        chart = (
            alt.Chart(chart_df)
            .mark_line()
            .encode(
                x=alt.X('time_slot:N', sort=None, scale=alt.Scale(domain=time_labels), axis=alt.Axis(values=hour_ticks, title='time_slot')),
                y=alt.Y('price:Q', scale=alt.Scale(domain=[0, 1500]), title='price'),
                color=alt.Color('model:N', title='model')
            )
            .properties(height=300)
        )
        _altair_chart(chart, width="stretch")


else:
    st.stop()
